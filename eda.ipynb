{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIuHXUTQFS0L"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_xsROLBFK5S"
      },
      "outputs": [],
      "source": [
        "%pip install ydata-profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jti8tcEBFXUW"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMY1z89GnT-1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNpe8udnFaYU"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps5QviADnbg2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40idf6GINzP8"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocrZ7vHBFouf"
      },
      "outputs": [],
      "source": [
        "report = ProfileReport(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic6X46zaFyfn"
      },
      "outputs": [],
      "source": [
        "report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHW3OFjyYn7D"
      },
      "outputs": [],
      "source": [
        "for c in df.columns:\n",
        "  if df[c].nunique() == len(df):\n",
        "    print(c, df[c].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9llFg1Xb5-8"
      },
      "source": [
        "There are 2 columns that have high correlation with each other. These can cause multicolinearity issues, so we should treat them carefully, because they can influence convergence of some training algorithms. But since there are only 2 such features and we use boosting algorithm, we can leave them, since they still can have explanatory power and these types of algorithms handle such features better than e.g. linear regression.\n",
        "\n",
        "\n",
        "There are also considerable amount of variable that have unique values including target. Normally, this could raise suspisions, since it can be some kind of ID that ended up in feature set accidentally. ID like columns don't carry any information that is useful for modelling, so in this case they should be discarded. However, here all of such columns are real and hence are not ID, so they can carry some information and should not be discarded.\n",
        "\n",
        "\n",
        "All the distributions look uniform and don't raise any suspicions.\n",
        "Correlations with the target are all pretty weak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_RIach1nwOZ"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgGF6m_Pcwc4"
      },
      "source": [
        "All columns are numeric. There is one boolean column, but it's probably fine to treat it as numeric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X_qctIRYt_Z"
      },
      "source": [
        "## Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeixQ95FYvnm"
      },
      "source": [
        "### Numeric vs Numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U03_HLgQYtld"
      },
      "outputs": [],
      "source": [
        "def plot_numerical_correlation_heatmap(d, method='pearson'):\n",
        "  # Compute the correlation matrix\n",
        "  corr = d.corr(method=method)\n",
        "\n",
        "  # Generate a mask for the upper triangle\n",
        "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8fwzrkVrsJY"
      },
      "outputs": [],
      "source": [
        "plot_numerical_correlation_heatmap(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfkQM9-0r9hN"
      },
      "outputs": [],
      "source": [
        "plot_numerical_correlation_heatmap(df, method='spearman')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofH9wr2LadKP"
      },
      "outputs": [],
      "source": [
        "df['8'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKaTD6J4aBwH"
      },
      "source": [
        "There is only one pair of variables that have high correlation. Some models can suffer issues with convergence when there is multicollinearity present. We can try to remove one of these features. Since one of these features is boolean, I'd probably remove this one, because it carries less information than second feature that is a real variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXd8VAUCZ0Q6"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ATwhig6a-AP"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['target'])\n",
        "y = df['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oza7oAWam-vc"
      },
      "source": [
        "Split dataset into train/val."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzKNuKISnyJk"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jxQnO6-trv8"
      },
      "outputs": [],
      "source": [
        "def modelfit(alg, dtrain, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "\n",
        "    if useTrainCV:\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(dtrain.values, label=target.values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
        "            metrics=['rmse'], early_stopping_rounds=early_stopping_rounds)\n",
        "        alg.set_params(n_estimators=cvresult.shape[0])\n",
        "\n",
        "        print(cvresult)\n",
        "\n",
        "    alg.fit(dtrain, target)\n",
        "\n",
        "    #Predict training set:\n",
        "    dtrain_predictions = alg.predict(dtrain)\n",
        "\n",
        "    #Print model report:\n",
        "    print(\"\\nModel Report\")\n",
        "    print(\"Train RMSE : %.4g\" % root_mean_squared_error(target.values, dtrain_predictions))\n",
        "\n",
        "    if useTrainCV:\n",
        "      print(\"Cross Validation RMSE: \", min(cvresult['test-rmse-mean']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwI74gWjSbFl"
      },
      "outputs": [],
      "source": [
        "def grid_search_validation(X, y, X_val, y_val, model, param_grid, score_func=root_mean_squared_error):\n",
        "  params, test_score = [], []\n",
        "  for param in param_grid:\n",
        "    model.set_params(**param)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    pred = model.predict(X_val)\n",
        "    score = score_func(y_val, pred)\n",
        "\n",
        "    params.append(param)\n",
        "    test_score.append(score)\n",
        "\n",
        "  return {'test_score': np.array(test_score),\n",
        "          'best_score': min(test_score),\n",
        "          'params': params,\n",
        "          'best_params': params[np.argmin(test_score)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufwQRpUAl8vp"
      },
      "outputs": [],
      "source": [
        "def plot_hyperparameter_results(cv_results, figsize=(25, 7)):\n",
        "  test_score = cv_results['test_score']\n",
        "  X = np.arange(len(test_score))\n",
        "\n",
        "  _, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  min_ind = np.argmin(test_score)\n",
        "\n",
        "  ax.set_title(f\"Best value:{np.round(test_score[min_ind], decimals=3)}\")\n",
        "  ax.set_xlabel(\"Parameter values\")\n",
        "  ax.set_ylabel(\"Score\")\n",
        "  ax.set_xticks(X)\n",
        "\n",
        "  labels = [str(x) for x in cv_results['params']]\n",
        "  ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "\n",
        "  [t.set_color('red')for i, t in enumerate(ax.xaxis.get_ticklabels()) if i == min_ind ]\n",
        "\n",
        "  ax.grid()\n",
        "  ax.plot(\n",
        "      X, test_score, \"o-\", color=\"blue\", label=\"CV Metric value\"\n",
        "  )\n",
        "\n",
        "  ax.plot(min_ind, test_score[min_ind], 'ro')\n",
        "  ax.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azoSuKkSKBd-"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9CyErCstfmQ"
      },
      "outputs": [],
      "source": [
        "xgb1 = xgb.XGBRegressor(\n",
        " learning_rate=0.1,\n",
        " n_estimators=1000,\n",
        " nthread=4,\n",
        " seed=27)\n",
        "\n",
        "X_train_c, y_train_c, X_val_c, y_val_c = X_train.copy(deep=True), \\\n",
        "y_train.copy(deep=True), X_val.copy(deep=True), y_val.copy(deep=True),\\\n",
        "\n",
        "\n",
        "modelfit(xgb1, X_train_c, target=y_train_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add some regularization to prevent possible overfit"
      ],
      "metadata": {
        "id": "69xrbAcHWz1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb2 = xgb.XGBRegressor(\n",
        " learning_rate=0.1,\n",
        " n_estimators=1000,\n",
        " reg_lambda=1e1,\n",
        " nthread=4,\n",
        " seed=27)\n",
        "\n",
        "X_train_c, y_train_c, X_val_c, y_val_c = X_train.copy(deep=True), \\\n",
        "y_train.copy(deep=True), X_val.copy(deep=True), y_val.copy(deep=True),\\\n",
        "\n",
        "\n",
        "modelfit(xgb2, X_train_c, target=y_train_c)"
      ],
      "metadata": {
        "id": "KYQWT1eJTivt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsfbOXGJxl4M"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJRnsZbO0TGy"
      },
      "source": [
        "Strategy here is to tune small groups of parameters (typically 1-2 parameters at once) from most important to least important ones. Sometimes we will tune one group more than once to explore space near previous optimal value in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss1Xo032vyFw"
      },
      "outputs": [],
      "source": [
        "param_test = {\n",
        " 'max_depth':range(1,10,2),\n",
        " 'min_child_weight':range(1,10,2),\n",
        "}\n",
        "\n",
        "res = grid_search_validation(X_train, y_train,\n",
        "                             X_val, y_val, xgb.XGBRegressor(learning_rate=0.1, n_estimators=200, reg_lambda=1e2, seed=27),\n",
        "                                   ParameterGrid(param_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seW31W5PR-XR"
      },
      "outputs": [],
      "source": [
        "plot_hyperparameter_results(res, figsize=(35, 7))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_test = {\n",
        " 'n_estimators': [200, 500, 700, 1000],\n",
        " 'learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "res = grid_search_validation(X_train, y_train,\n",
        "                             X_val, y_val, xgb.XGBRegressor(reg_lambda=1e2, max_depth=9, min_child_weight=7, seed=27),\n",
        "                                   ParameterGrid(param_test))"
      ],
      "metadata": {
        "id": "JICmhKhAW7Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hyperparameter_results(res, figsize=(35, 7))"
      ],
      "metadata": {
        "id": "O0G2FwVtZWyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results suggest that xgboost is a good choice in this scenario. Now we can train it on the whole training dataset with parameters we have identified during this EDA. We didn't try many combinations, but current results are firstly good enough and secondly pretty stable, so other values will probably produce similar validation metric values."
      ],
      "metadata": {
        "id": "L50bF_HKsOqJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}